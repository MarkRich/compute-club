{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from LanguageIdentification import LanguageIdentification\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProfanityPredictor import ProfanityPredictor\n",
    "\n",
    "sample_docs = ['hello there', 'السلام عليكم', 'hello there', 'fuck you']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:  /Users/abdul/Desktop/compute-club-fork/filtering/models/lid.176.bin\n"
     ]
    }
   ],
   "source": [
    "LanguageIdModel = LanguageIdentification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc:  hello there\n",
      "lang:  ('en', 0.77) \n",
      "\n",
      "doc:  السلام عليكم\n",
      "lang:  ('ar', 0.98) \n",
      "\n",
      "doc:  hello there\n",
      "lang:  ('en', 0.77) \n",
      "\n",
      "doc:  fuck you\n",
      "lang:  ('en', 0.74) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in sample_docs:\n",
    "  print('doc: ', doc)\n",
    "  print('lang: ', LanguageIdModel.predict_lang(doc), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profanity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_modal = ProfanityPredictor()\n",
    "prof_scores = prof_modal.predict(sample_docs)\n",
    "profanity_data = zip(sample_docs, prof_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:  ('hello there', 0.03218035443843418)\n",
      "d:  ('السلام عليكم', 0.045604353111609396)\n",
      "d:  ('hello there', 0.03218035443843418)\n",
      "d:  ('fuck you', 0.9999999997233655)\n"
     ]
    }
   ],
   "source": [
    "for d in profanity_data:\n",
    "    print('d: ', d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplication\n",
    "\n",
    "We are using the [text-dedup](https://chenghaomou.github.io/text-dedup/) library, which has a collection of deduplication scripts that runs on huggingFace datasets and local datasets.\n",
    "\n",
    "**Setup huggingface**\n",
    "1. Login to https://huggingface.co\n",
    "2. Go to https://huggingface.co/settings/tokens\n",
    "3. Create a new token and copy it\n",
    "4. In your terminal, run `huggingface-cli login`\n",
    "5. Paste the token you copied when prompted\n",
    "\n",
    "**Selecting dataset**\n",
    "`--path` can point to a huggingface dataset or a local dataset. The script will automatically download the dataset if it is a huggingface dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some examples of how to deduplicate data\n",
    "# python -m text_dedup.minhash \\\n",
    "#    --path \"oscar-corpus/OSCAR-2201\" \\\n",
    "#    --name \"gl\" \\\n",
    "#    --split \"train\" \\\n",
    "#    --cache_dir \"filtered_datasets/oscar-corpus/cache\" \\\n",
    "#    --output \"output/minhash/oscar_gl_dedup\" \\\n",
    "#    --column \"text\" \\\n",
    "#    --batch_size 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO] Rules/Heuristics**\n",
    "\n",
    "not that critical as we aren't parsing data from those URLs, and not the pile\n",
    "maybe just someone mentions them in some conversation?\n",
    "- banned URLs: https://dsi.ut-capitole.fr/blacklists/index_en.php\n",
    "- banned expressions - data-set TBD\n",
    "- trafilatura -- ignore menus...etc not needed\n",
    "- rules/heuristics - punc, sybols, banned-words\n",
    "- ml-based quality filterin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
