model_path: "NousResearch/Llama-2-13b-hf"
input_dataset_path: "AlekseyKorshuk/hh-lmgym-demo"
output_dataset_path: "rrustom/gptj-qlora-peft"
verbose: True

lora_args:
  r: 8
  lora_alpha: 32
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

training_args:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 4
  warmup_steps: 2
  max_steps: 10
  learning_rate: 0.0002
  fp16: True
  logging_steps: 1
  optimizer: "paged_adamw_8bit"
